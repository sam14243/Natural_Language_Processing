{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xMMJCsqnOUDx",
        "outputId": "8d546261-28d3-4252-e84c-e8d5f5fa1332"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "fatal: destination path 'Tamil-English-Dataset' already exists and is not an empty directory.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/Ishikahooda/Tamil-English-Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "Q8wB9447ONQR"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [],
      "source": [
        "with open(\"Tamil-English-Dataset/Dataset/data.en1\") as f:\n",
        "    data_en = f.readlines()\n",
        "    data_en = data_en[:1500]\n",
        "\n",
        "with open(\"Tamil-English-Dataset/Dataset/data.ta1\") as f:\n",
        "    data_ta = f.readlines()\n",
        "    data_ta = data_ta[:1500]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2000615\n",
            "615\n"
          ]
        }
      ],
      "source": [
        "import fasttext\n",
        "\n",
        "# Convert the data_ta list to a text file\n",
        "with open('data_ta.txt', 'w', encoding='utf-8') as f:\n",
        "    f.writelines(data_ta)\n",
        "\n",
        "# Train the FastText model\n",
        "model_ta = fasttext.train_unsupervised('data_ta.txt', model='skipgram')\n",
        "\n",
        "# Get the word vectors\n",
        "word_vectors = model_ta.get_input_matrix()\n",
        "print(len(word_vectors))\n",
        "\n",
        "# Get the word index\n",
        "word_index = model_ta.get_words()\n",
        "print(len(word_index))\n",
        "# Create a dictionary with the word and the corresponding vector\n",
        "word_vector_dict = {}\n",
        "for word, vector in zip(word_index, word_vectors):\n",
        "    word_vector_dict[word] = vector\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "615"
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(word_vector_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Convert the data_ta list to a text file\n",
        "with open('data_en.txt', 'w', encoding='utf-8') as f:\n",
        "    f.writelines(data_en)\n",
        "\n",
        "# Train the FastText model\n",
        "model_en = fasttext.train_unsupervised('data_en.txt', model='skipgram')\n",
        "\n",
        "# Get the word vectors\n",
        "word_vectors = model_en.get_input_matrix()\n",
        "\n",
        "# Get the word index\n",
        "word_index = model_en.get_words()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tB4vacZiONQT",
        "outputId": "5de95620-8e8f-4006-d9f7-9466dccee799"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Lohith\\miniconda3\\envs\\tensor\\lib\\site-packages\\ipykernel_launcher.py:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  \n"
          ]
        }
      ],
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import numpy as np\n",
        "\n",
        "def pad_sequences(data, maxlen):\n",
        "    data = np.array(data)\n",
        "    data = np.array(list(map(lambda x: x + ['<PAD>'] * (maxlen - len(x)), data)))\n",
        "    return data\n",
        "\n",
        "data_ta_split = list(map(lambda x: x.split(), data_ta))\n",
        "data_ta_pad = pad_sequences(data_ta_split, maxlen=69)\n",
        "\n",
        "data_en_split = list(map(lambda x: x.split(), data_en))\n",
        "data_en_pad = pad_sequences(data_en_split, maxlen=69)\n",
        "\n",
        "\n",
        "def get_embed(model,sentence):\n",
        "    embed = []\n",
        "    for word in sentence:\n",
        "        embed.append(model.get_word_vector(word))\n",
        "\n",
        "    embed = np.stack(embed, axis=0).astype(np.float32)\n",
        "    return embed\n",
        "\n",
        "ta_embed = []\n",
        "for sentence in data_ta_pad:\n",
        "    ta_embed += [get_embed(model_ta,sentence)]\n",
        "\n",
        "en_embed = []\n",
        "for sentence in data_en_pad:\n",
        "    en_embed += [get_embed(model_en,sentence)]\n",
        "\n",
        "\n",
        "\n",
        "ta_embed = np.stack(ta_embed, axis=0).astype(np.float32)\n",
        "en_embed = np.stack(en_embed, axis=0).astype(np.float32)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((1500, 69, 100), (1500, 69, 100))"
            ]
          },
          "execution_count": 45,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ta_embed.shape, en_embed.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm_8 (LSTM)               (None, 69, 128)           117248    \n",
            "                                                                 \n",
            " lstm_9 (LSTM)               (None, 69, 128)           131584    \n",
            "                                                                 \n",
            " lstm_10 (LSTM)              (None, 69, 128)           131584    \n",
            "                                                                 \n",
            " lstm_11 (LSTM)              (None, 69, 128)           131584    \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 69, 100)           12900     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 524,900\n",
            "Trainable params: 524,900\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/30\n",
            "24/24 [==============================] - 4s 31ms/step - loss: 0.0035 - accuracy: 0.1531\n",
            "Epoch 2/30\n",
            "24/24 [==============================] - 1s 32ms/step - loss: 0.0020 - accuracy: 0.3304\n",
            "Epoch 3/30\n",
            "24/24 [==============================] - 1s 31ms/step - loss: 0.0017 - accuracy: 0.3619\n",
            "Epoch 4/30\n",
            "24/24 [==============================] - 1s 32ms/step - loss: 0.0016 - accuracy: 0.3619\n",
            "Epoch 5/30\n",
            "24/24 [==============================] - 1s 31ms/step - loss: 0.0015 - accuracy: 0.3734\n",
            "Epoch 6/30\n",
            "24/24 [==============================] - 1s 30ms/step - loss: 0.0014 - accuracy: 0.3418\n",
            "Epoch 7/30\n",
            "24/24 [==============================] - 1s 32ms/step - loss: 0.0015 - accuracy: 0.3453\n",
            "Epoch 8/30\n",
            "24/24 [==============================] - 1s 32ms/step - loss: 0.0014 - accuracy: 0.3334\n",
            "Epoch 9/30\n",
            "24/24 [==============================] - 1s 31ms/step - loss: 0.0014 - accuracy: 0.3355\n",
            "Epoch 10/30\n",
            "24/24 [==============================] - 1s 32ms/step - loss: 0.0014 - accuracy: 0.3375\n",
            "Epoch 11/30\n",
            "24/24 [==============================] - 1s 31ms/step - loss: 0.0013 - accuracy: 0.3462\n",
            "Epoch 12/30\n",
            "24/24 [==============================] - 1s 31ms/step - loss: 0.0013 - accuracy: 0.3364\n",
            "Epoch 13/30\n",
            "24/24 [==============================] - 1s 30ms/step - loss: 0.0013 - accuracy: 0.3722\n",
            "Epoch 14/30\n",
            "24/24 [==============================] - 1s 31ms/step - loss: 0.0013 - accuracy: 0.3418\n",
            "Epoch 15/30\n",
            "24/24 [==============================] - 1s 30ms/step - loss: 0.0013 - accuracy: 0.3371\n",
            "Epoch 16/30\n",
            "24/24 [==============================] - 1s 30ms/step - loss: 0.0013 - accuracy: 0.3367\n",
            "Epoch 17/30\n",
            "24/24 [==============================] - 1s 30ms/step - loss: 0.0013 - accuracy: 0.3422\n",
            "Epoch 18/30\n",
            "24/24 [==============================] - 1s 30ms/step - loss: 0.0013 - accuracy: 0.3378\n",
            "Epoch 19/30\n",
            "24/24 [==============================] - 1s 30ms/step - loss: 0.0013 - accuracy: 0.3370\n",
            "Epoch 20/30\n",
            "24/24 [==============================] - 1s 29ms/step - loss: 0.0013 - accuracy: 0.3782\n",
            "Epoch 21/30\n",
            "24/24 [==============================] - 1s 30ms/step - loss: 0.0013 - accuracy: 0.3610\n",
            "Epoch 22/30\n",
            "24/24 [==============================] - 1s 30ms/step - loss: 0.0013 - accuracy: 0.3386\n",
            "Epoch 23/30\n",
            "24/24 [==============================] - 1s 29ms/step - loss: 0.0013 - accuracy: 0.3404\n",
            "Epoch 24/30\n",
            "24/24 [==============================] - 1s 30ms/step - loss: 0.0013 - accuracy: 0.3338\n",
            "Epoch 25/30\n",
            "24/24 [==============================] - 1s 30ms/step - loss: 0.0013 - accuracy: 0.3329\n",
            "Epoch 26/30\n",
            "24/24 [==============================] - 1s 30ms/step - loss: 0.0013 - accuracy: 0.3351\n",
            "Epoch 27/30\n",
            "24/24 [==============================] - 1s 30ms/step - loss: 0.0012 - accuracy: 0.3459\n",
            "Epoch 28/30\n",
            "24/24 [==============================] - 1s 31ms/step - loss: 0.0012 - accuracy: 0.3602\n",
            "Epoch 29/30\n",
            "24/24 [==============================] - 1s 31ms/step - loss: 0.0013 - accuracy: 0.3424\n",
            "Epoch 30/30\n",
            "24/24 [==============================] - 1s 30ms/step - loss: 0.0013 - accuracy: 0.3356\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x1fb6bbd5ec8>"
            ]
          },
          "execution_count": 46,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, LSTM, Dense, RepeatVector\n",
        "\n",
        "model = Sequential()\n",
        "model.add(LSTM(128, return_sequences=True))\n",
        "model.add(LSTM(128, return_sequences=True))\n",
        "model.add(LSTM(128, return_sequences=True))\n",
        "model.add(LSTM(128, return_sequences=True))\n",
        "model.add(Dense(100, activation='elu'))\n",
        "model.compile(optimizer='adam', loss='mean_squared_error', metrics=['accuracy'])\n",
        "model.build(input_shape=(None, 69, 100))\n",
        "model.summary()\n",
        "\n",
        "model.fit(ta_embed, en_embed, epochs=30, batch_size=64)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 1s 1s/step\n",
            "Input: இருவித ஊதிய விகிதங்களும் ஏற்கப்பட்டன மேற்கு ஐரோப்பிய துறைமுகப் பணியாளர்களுக்கும் கிழக்கு ஐரோப்பிய நாடுகள் , ஆசியா இவற்றின் தொழிலாளர்களுக்கும் இடையே பிளவுகளைக் கொண்டுவரும் துறைமுகத் தொழிற்சங்கங்களின் கொள்கை இதே பேரழிவு விளைவுகளைத்தான் கொண்டுவரும் .\n",
            "Predicted: further children flesh destroy destroy without destroy destroy offering without destroy destroy destroy destroy destroy destroy destroy destroy destroy destroy destroy destroy destroy offering destroy destroy\n"
          ]
        }
      ],
      "source": [
        "# Get the sample input from data_ta\n",
        "sample_input = data_ta[10]\n",
        "sample_input = sample_input.split()\n",
        "\n",
        "# Get the word vectors for the sample input\n",
        "sample_input_embed = get_embed(model_ta, sample_input)\n",
        "\n",
        "# Get the prediction\n",
        "prediction = model.predict(np.expand_dims(sample_input_embed, axis=0))\n",
        "\n",
        "def get_word(vector, model):\n",
        "    vectors = [model.get_word_vector(word) for word in model.get_words()]\n",
        "    vectors = np.stack(vectors, axis=0).astype(np.float32)\n",
        "\n",
        "    # Calculate the cosine similarity\n",
        "    similarity = vectors @ vector / (np.linalg.norm(vectors, axis=1) * np.linalg.norm(vector))\n",
        "\n",
        "\n",
        "    # Get the index of the most similar word\n",
        "    most_similar_index = np.argmax(similarity)\n",
        "\n",
        "    # Get the word\n",
        "    words = model_en.get_words()\n",
        "    most_similar_word = words[most_similar_index]\n",
        "\n",
        "    return most_similar_word\n",
        "\n",
        "# Get the predicted sentence\n",
        "predicted_sentence = []\n",
        "for word in prediction[0]:\n",
        "    predicted_sentence.append(get_word(word, model_en))\n",
        "\n",
        "predicted_sentence = ' '.join(predicted_sentence)\n",
        "\n",
        "print(f\"Input: {' '.join(sample_input)}\")\n",
        "print(f\"Predicted: {predicted_sentence}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "further children flesh destroy destroy without destroy destroy offering without destroy destroy destroy destroy destroy destroy destroy destroy destroy destroy destroy destroy destroy offering destroy destroy\n"
          ]
        }
      ],
      "source": [
        "print(predicted_sentence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
