{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.16"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 533474,
          "sourceType": "datasetVersion",
          "datasetId": 30764
        }
      ],
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "name": "Sarcasm Detection [train = 99,9% , Testing = 81%]",
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "source": [
        "\n",
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES\n",
        "# TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "import os\n",
        "import sys\n",
        "from tempfile import NamedTemporaryFile\n",
        "from urllib.request import urlopen\n",
        "from urllib.parse import unquote, urlparse\n",
        "from urllib.error import HTTPError\n",
        "from zipfile import ZipFile\n",
        "import tarfile\n",
        "import shutil\n",
        "\n",
        "CHUNK_SIZE = 40960\n",
        "DATA_SOURCE_MAPPING = 'news-headlines-dataset-for-sarcasm-detection:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F30764%2F533474%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240410%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240410T074355Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D0180fb9481deb22c5a33ecdd69543d19c59d23b093c7a8e9878e5384f8b5cfd366fde45c7676380c62f5d2bc86872fded80cf64b578ca82bf0e66e99fb298be55c595102ba95430fd176b63af7ebc8212dc3307ad13130e1670df585de4cc3e7af938f4978446e3ccf7419033aa3de93197f356ef162f2ad6d0efdef8435055c5f35300cef0f37bc7b65fbd63d1ae7883d16aa6e428e40312f67151162e0f0f676bb18fca77d38a190de25b607e2360887babdf67b708be6bfe4b256de95227087c05dcfa0aad1cc4939adfdb9680d9958af7fdfd69b15868406db3966247feafc7adf7a80a025cfe1262b93dec1ab36847475ed1c44547bbd43d9e522a09a95'\n",
        "\n",
        "KAGGLE_INPUT_PATH='/kaggle/input'\n",
        "KAGGLE_WORKING_PATH='/kaggle/working'\n",
        "KAGGLE_SYMLINK='kaggle'\n",
        "\n",
        "!umount /kaggle/input/ 2> /dev/null\n",
        "shutil.rmtree('/kaggle/input', ignore_errors=True)\n",
        "os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)\n",
        "os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)\n",
        "\n",
        "try:\n",
        "  os.symlink(KAGGLE_INPUT_PATH, os.path.join(\"..\", 'input'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "try:\n",
        "  os.symlink(KAGGLE_WORKING_PATH, os.path.join(\"..\", 'working'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "\n",
        "for data_source_mapping in DATA_SOURCE_MAPPING.split(','):\n",
        "    directory, download_url_encoded = data_source_mapping.split(':')\n",
        "    download_url = unquote(download_url_encoded)\n",
        "    filename = urlparse(download_url).path\n",
        "    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)\n",
        "    try:\n",
        "        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:\n",
        "            total_length = fileres.headers['content-length']\n",
        "            print(f'Downloading {directory}, {total_length} bytes compressed')\n",
        "            dl = 0\n",
        "            data = fileres.read(CHUNK_SIZE)\n",
        "            while len(data) > 0:\n",
        "                dl += len(data)\n",
        "                tfile.write(data)\n",
        "                done = int(50 * dl / int(total_length))\n",
        "                sys.stdout.write(f\"\\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded\")\n",
        "                sys.stdout.flush()\n",
        "                data = fileres.read(CHUNK_SIZE)\n",
        "            if filename.endswith('.zip'):\n",
        "              with ZipFile(tfile) as zfile:\n",
        "                zfile.extractall(destination_path)\n",
        "            else:\n",
        "              with tarfile.open(tfile.name) as tarfile:\n",
        "                tarfile.extractall(destination_path)\n",
        "            print(f'\\nDownloaded and uncompressed: {directory}')\n",
        "    except HTTPError as e:\n",
        "        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')\n",
        "        continue\n",
        "    except OSError as e:\n",
        "        print(f'Failed to load {download_url} to path {destination_path}')\n",
        "        continue\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "U9EMZ7PPOQVr"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Main Library"
      ],
      "metadata": {
        "id": "HV_ZUbqWOQVv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Reading Data\n",
        "import json, os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Data Preprocessing\n",
        "import re, nltk, string\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Building Model\n",
        "from tensorflow.keras.layers import Dense, Embedding, LSTM, Bidirectional, GlobalAveragePooling1D\n",
        "import tensorflow.keras as k\n",
        "\n",
        "# Download Model\n",
        "import pickle"
      ],
      "metadata": {
        "id": "U3YsuokYOQVw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Reading Data"
      ],
      "metadata": {
        "id": "AAo2HX4IOQVx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# with open(r\"D:\\Courses language programming\\Natural Language Processing\\Projects For NLP\\Data\\News Headlines Dataset For Sarcasm Detection\\Sarcasm_Headlines_Dataset.json\", 'r') as f:\n",
        "#     datastore = json.load(f)\n",
        "\n",
        "# sentence = []\n",
        "# urls = []\n",
        "# labels = []\n",
        "\n",
        "# for item in datastore:\n",
        "#     sentence.append(item[\"headline\"])\n",
        "#     urls.append(item[\"atricle_link\"])\n",
        "#     labels.append(item[\"is_sarcastic\"])\n",
        "\n",
        "\n",
        "# sentense[:5]"
      ],
      "metadata": {
        "id": "tDZEaNbxOQVx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_json(r\"D:\\Courses language programming\\Natural Language Processing\\Projects For NLP\\Data\\News Headlines Dataset For Sarcasm Detection\\Sarcasm_Headlines_Dataset.json\", lines=True)\n",
        "data.head()"
      ],
      "metadata": {
        "id": "qqSp_MOgOQVx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.drop(columns=\"article_link\", axis=1, inplace=True)\n",
        "data.head(5)"
      ],
      "metadata": {
        "id": "RWZ7u80mOQVx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data[\"headline\"][0]"
      ],
      "metadata": {
        "id": "IV6a3J-iOQVy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.info()"
      ],
      "metadata": {
        "id": "aBOsnkxROQVy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Do not Have Null Value"
      ],
      "metadata": {
        "id": "4XYdBytkOQVy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing in Data"
      ],
      "metadata": {
        "id": "dX3KQS2wOQVz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Removing Stop Word And Punctuation And Make Lemmetization"
      ],
      "metadata": {
        "id": "1aFpLSYPOQVz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1 - Stop Words\n",
        "## 2 - Punctuation\n",
        "## 3 - Lemmatization"
      ],
      "metadata": {
        "id": "MdDKE-AkOQVz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "punc = list(string.punctuation)\n",
        "stop_words = stopwords.words(\"english\")\n",
        "lemma = WordNetLemmatizer()"
      ],
      "metadata": {
        "id": "xMra__M7OQVz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def Process(data):\n",
        "    data.lower()\n",
        "\n",
        "    data = \" \".join([lemma.lemmatize(word) for word in word_tokenize(data) if ((word not in punc) and (word not in stop_words))])\n",
        "\n",
        "    data = re.sub(\"[^a-z]\", \" \", data)\n",
        "\n",
        "    return data"
      ],
      "metadata": {
        "id": "fIOj1MQjOQVz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data[\"headline\"] = data[\"headline\"].apply(Process)\n",
        "data.head(5)"
      ],
      "metadata": {
        "id": "XUje5O7rOQV0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# To Categorical\n",
        "## To Make Transform From 1output Label --> Len*Ouput (One Hot Encoder)"
      ],
      "metadata": {
        "id": "8h1TT8K7OQV0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "label = to_categorical(data[\"is_sarcastic\"], 2)\n",
        "label[:5]"
      ],
      "metadata": {
        "id": "zdLpRF6iOQV0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Make Tokenization"
      ],
      "metadata": {
        "id": "KM0wyZFGOQV0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = data[\"headline\"]\n",
        "Y = label\n",
        "print(Y[:2])"
      ],
      "metadata": {
        "id": "m6e1wJiWOQV0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenize = Tokenizer(oov_token=\"<oov>\")\n",
        "tokenize.fit_on_texts(X)\n",
        "word_idx = tokenize.word_index\n",
        "\n",
        "data_seqence = tokenize.texts_to_sequences(X)\n",
        "pad_seq = pad_sequences(data_seqence, padding=\"pre\", truncating=\"pre\")\n",
        "\n",
        "print(\"The Padding Sequance Shape is  --> \", pad_seq.shape)"
      ],
      "metadata": {
        "id": "7cK5iz3QOQV0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_length = max(len(seq) for seq in data_seqence)\n",
        "\n",
        "vocabulary_size = len(word_idx) + 1\n",
        "\n",
        "input_length, vocabulary_size"
      ],
      "metadata": {
        "id": "FDZyFDgiOQV1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Splitting Data To Training And Testing"
      ],
      "metadata": {
        "id": "PvbH2snxOQV1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_train, x_test, y_train, y_test = train_test_split(pad_seq, label, train_size=0.7)"
      ],
      "metadata": {
        "id": "z-RFXyIyOQV1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Building Deep Learning Model"
      ],
      "metadata": {
        "id": "R_AEt07jOQV1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = k.models.Sequential([\n",
        "    Embedding(vocabulary_size, 50, input_length=input_length),\n",
        "    GlobalAveragePooling1D(),\n",
        "    Dense(48, activation=\"relu\"),\n",
        "    Dense(2, activation=\"softmax\")\n",
        "])\n",
        "\n",
        "model.compile(optimizer=\"adam\", loss=k.losses.BinaryCrossentropy(), metrics=[\"accuracy\"])"
      ],
      "metadata": {
        "id": "3BjuFIfEOQV1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(x_train, y_train, epochs=20, validation_data=(x_test, y_test), verbose=2)"
      ],
      "metadata": {
        "scrolled": true,
        "id": "3hVCSUlMOQV1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(history.history[\"loss\"], label=\"Loss\")\n",
        "plt.plot(history.history[\"val_loss\"], label=\"Val_Loss\")\n",
        "\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "\n",
        "plt.title(\"Loss Vs Epochs\")\n",
        "\n",
        "plt.legend()\n",
        "plt.grid()"
      ],
      "metadata": {
        "id": "KCz4nUL9OQV1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(history.history[\"accuracy\"], label=\"accuracy\")\n",
        "plt.plot(history.history[\"val_accuracy\"], label=\"val_accuracy\")\n",
        "\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "\n",
        "plt.title(\"Accuracy Vs Epochs\")\n",
        "\n",
        "plt.legend()\n",
        "plt.grid()"
      ],
      "metadata": {
        "id": "oq476arMOQV1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# System Prediction"
      ],
      "metadata": {
        "id": "HFACgBaVOQV1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = word_tokenize(input())\n",
        "\n",
        "new_text = \"\"\n",
        "for word in text:\n",
        "    if (word not in stop_words) and (word not in punc):\n",
        "        new_text += lemma.lemmatize(word)\n",
        "        new_text += \" \"\n",
        "\n",
        "print(new_text)\n",
        "test_sequace = tokenize.texts_to_sequences([new_text])\n",
        "test_padding = pad_sequences(test_sequace, maxlen=31, padding=\"pre\", truncating=\"pre\")\n",
        "\n",
        "\n",
        "# test_sequace\n",
        "prediction = model.predict(test_padding)\n",
        "\n",
        "print(prediction[0])\n",
        "if np.argmax(prediction) == 0: print(\"This Massage is -->  is_sarcastic \")\n",
        "else: print(\"This Massage is -->  not is_sarcastic \")"
      ],
      "metadata": {
        "id": "B-shblmsOQV2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download Model"
      ],
      "metadata": {
        "id": "YeR5bq3WOQV2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pickle.dump(model, open(r\"D:\\Pycharm\\model_pickle\\NLP - Models\\\\Sarcasm Detection.bin\", \"wb\"))"
      ],
      "metadata": {
        "id": "h7mf8NqqOQV8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NbiGbfOiOQV9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}