{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/achintya/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/achintya/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import spacy\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk.tokenize import sent_tokenize , word_tokenize \n",
    "import string\n",
    "import numpy as np\n",
    "import re\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer , TfidfVectorizer\n",
    "from gensim.models import Word2Vec\n",
    "import textwrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords =  stopwords.words('english')\n",
    "lem  = WordNetLemmatizer()\n",
    "NUM_CLUSTERS=15\n",
    "iterations=25\n",
    "text = \"\"\"Millions go missing at China bank Two senior officials at one of China's top commercial banks have reportedly disappeared after funds worth up to $120m (Â£64m) went missing. The pair both worked at Bank of China in the northern city of Harbin, the South China Morning Post reported. The latest scandal at Bank of China will do nothing to reassure foreign investors that China's big four banks are ready for international listings. Government policy sees the bank listings as vital economic reforms. Bank of China is one of two frontrunners in the race to list overseas. The other is China Construction Bank. Both are expected to list abroad during 2005. They shared a $45bn state bailout in 2003, to help clean up their balance sheets in preparation for a foreign stock market debut. However, a report in the China-published Economic Observer said on Monday that the two banks may have scrapped plans to list in New York because of the cost of meeting regulatory requirements imposed since the Enron scandal. Bank of China is the country's biggest foreign exchange dealer, while China Construction Bank is the largest deposit holder. China's banking sector is burdened with at least $190bn of bad debt according to official data, though most observers believe the true figure is far higher. Officially, one in five loans is not being repaid. Attempts to strengthen internal controls an tighten lending policies have uncovered a succession of scandals involving embezzlement by bank officials and loans-for-favours. The most high-profile case involved the ex-president of Bank of China, Wang Xuebing, jailed for 12 years in 2003. Although, he committed the offences whilst running Bank of China in New York, Mr.Wang was head of China Construction Bank when the scandal broke. Earlier this month, a China Construction Bank branch manager was jailed for life in a separate case. China's banks used to act as cash offices for state enterprises and did not require checks on credit worthiness. The introduction of market reforms has been accompanied by attempts to modernize the banking sector, but links between banks and local government remain strong. Last year, China's premier, Wen Jiabao, targeted bank lending practices in a series of speeches, and regulators ordered all big loans to be scrutinized, in an attempt to cool down irresponsible lending. China's leaders see reforming the top four banks as vital to distribute capital to profitable companies and protect the health of China's economic boom. But two problems persist. First, inefficient state enterprises continue to receive protection from bankruptcy because they employ large numbers of people. Second, many questionable loans come not from the big four, but from smaller banks. Another high-profile financial firm, China Life, is facing shareholder lawsuits and a probe by the US Securities and Exchange Commission following its 2004 New York listing over its failure to disclose accounting irregularities at its parent company.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_word_vectors(tokens, model, vector_size):\n",
    "    vector_sum = np.zeros(vector_size)\n",
    "    count = 0\n",
    "    for word in tokens:\n",
    "        if word in model.wv:\n",
    "            vector_sum += model.wv[word]\n",
    "            count += 1\n",
    "    if count != 0:\n",
    "        return vector_sum / count\n",
    "    else:\n",
    "            return np.zeros(vector_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def preprocess(text):\n",
    "    words = word_tokenize(text)\n",
    "    words_lower = [word.lower() for word in words ]\n",
    "    words_strip = [word.strip() for word in words_lower] \n",
    "    words_remove_stopwords = [word for word in words_strip if word not in stopwords]\n",
    "    words_remove_punc = [word for word in words_remove_stopwords if\n",
    "    word not in string.punctuation]\n",
    "    words_lemmatised = [lem.lemmatize(word) for word in words_remove_punc]\n",
    "    words_joined = \" \".join(words_lemmatised)\n",
    "    return words_joined\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Original</th>\n",
       "      <th>Preprocessed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Millions go missing at China bank Two senior o...</td>\n",
       "      <td>million go missing china bank two senior offic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The pair both worked at Bank of China in the n...</td>\n",
       "      <td>pair worked bank china northern city harbin so...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The latest scandal at Bank of China will do no...</td>\n",
       "      <td>latest scandal bank china nothing reassure for...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Government policy sees the bank listings as vi...</td>\n",
       "      <td>government policy see bank listing vital econo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bank of China is one of two frontrunners in th...</td>\n",
       "      <td>bank china one two frontrunners race list over...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>The other is China Construction Bank.</td>\n",
       "      <td>china construction bank</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Both are expected to list abroad during 2005.</td>\n",
       "      <td>expected list abroad 2005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>They shared a $45bn state bailout in 2003, to ...</td>\n",
       "      <td>shared 45bn state bailout 2003 help clean bala...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>However, a report in the China-published Econo...</td>\n",
       "      <td>however report china-published economic observ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Bank of China is the country's biggest foreign...</td>\n",
       "      <td>bank china country 's biggest foreign exchange...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>China's banking sector is burdened with at lea...</td>\n",
       "      <td>china 's banking sector burdened least 190bn b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Officially, one in five loans is not being rep...</td>\n",
       "      <td>officially one five loan repaid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Attempts to strengthen internal controls an ti...</td>\n",
       "      <td>attempt strengthen internal control tighten le...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>The most high-profile case involved the ex-pre...</td>\n",
       "      <td>high-profile case involved ex-president bank c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Although, he committed the offences whilst run...</td>\n",
       "      <td>although committed offence whilst running bank...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Earlier this month, a China Construction Bank ...</td>\n",
       "      <td>earlier month china construction bank branch m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>China's banks used to act as cash offices for ...</td>\n",
       "      <td>china 's bank used act cash office state enter...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>The introduction of market reforms has been ac...</td>\n",
       "      <td>introduction market reform accompanied attempt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Last year, China's premier, Wen Jiabao, target...</td>\n",
       "      <td>last year china 's premier wen jiabao targeted...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>China's leaders see reforming the top four ban...</td>\n",
       "      <td>china 's leader see reforming top four bank vi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>But two problems persist.</td>\n",
       "      <td>two problem persist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>First, inefficient state enterprises continue ...</td>\n",
       "      <td>first inefficient state enterprise continue re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Second, many questionable loans come not from ...</td>\n",
       "      <td>second many questionable loan come big four sm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Another high-profile financial firm, China Lif...</td>\n",
       "      <td>another high-profile financial firm china life...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Original  \\\n",
       "0   Millions go missing at China bank Two senior o...   \n",
       "1   The pair both worked at Bank of China in the n...   \n",
       "2   The latest scandal at Bank of China will do no...   \n",
       "3   Government policy sees the bank listings as vi...   \n",
       "4   Bank of China is one of two frontrunners in th...   \n",
       "5               The other is China Construction Bank.   \n",
       "6       Both are expected to list abroad during 2005.   \n",
       "7   They shared a $45bn state bailout in 2003, to ...   \n",
       "8   However, a report in the China-published Econo...   \n",
       "9   Bank of China is the country's biggest foreign...   \n",
       "10  China's banking sector is burdened with at lea...   \n",
       "11  Officially, one in five loans is not being rep...   \n",
       "12  Attempts to strengthen internal controls an ti...   \n",
       "13  The most high-profile case involved the ex-pre...   \n",
       "14  Although, he committed the offences whilst run...   \n",
       "15  Earlier this month, a China Construction Bank ...   \n",
       "16  China's banks used to act as cash offices for ...   \n",
       "17  The introduction of market reforms has been ac...   \n",
       "18  Last year, China's premier, Wen Jiabao, target...   \n",
       "19  China's leaders see reforming the top four ban...   \n",
       "20                          But two problems persist.   \n",
       "21  First, inefficient state enterprises continue ...   \n",
       "22  Second, many questionable loans come not from ...   \n",
       "23  Another high-profile financial firm, China Lif...   \n",
       "\n",
       "                                         Preprocessed  \n",
       "0   million go missing china bank two senior offic...  \n",
       "1   pair worked bank china northern city harbin so...  \n",
       "2   latest scandal bank china nothing reassure for...  \n",
       "3   government policy see bank listing vital econo...  \n",
       "4   bank china one two frontrunners race list over...  \n",
       "5                             china construction bank  \n",
       "6                           expected list abroad 2005  \n",
       "7   shared 45bn state bailout 2003 help clean bala...  \n",
       "8   however report china-published economic observ...  \n",
       "9   bank china country 's biggest foreign exchange...  \n",
       "10  china 's banking sector burdened least 190bn b...  \n",
       "11                    officially one five loan repaid  \n",
       "12  attempt strengthen internal control tighten le...  \n",
       "13  high-profile case involved ex-president bank c...  \n",
       "14  although committed offence whilst running bank...  \n",
       "15  earlier month china construction bank branch m...  \n",
       "16  china 's bank used act cash office state enter...  \n",
       "17  introduction market reform accompanied attempt...  \n",
       "18  last year china 's premier wen jiabao targeted...  \n",
       "19  china 's leader see reforming top four bank vi...  \n",
       "20                                two problem persist  \n",
       "21  first inefficient state enterprise continue re...  \n",
       "22  second many questionable loan come big four sm...  \n",
       "23  another high-profile financial firm china life...  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "sentences = sent_tokenize(text)\n",
    "sentence_df = pd.DataFrame(sentences)\n",
    "sentence_df.columns = ['Original']\n",
    "sentence_df['Preprocessed'] = sentence_df['Original'].apply(preprocess)\n",
    "sentence_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer_bow = CountVectorizer(max_features=1000)\n",
    "X_bow = vectorizer_bow.fit_transform(sentence_df['Preprocessed']).toarray()\n",
    "vectorizer_tfidf = TfidfVectorizer()\n",
    "X_tfidf = vectorizer_tfidf.fit_transform(sentence_df['Preprocessed']).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import requests, zipfile, io\n",
    "\n",
    "# def download_glove():\n",
    "#     glove_url = \"http://nlp.stanford.edu/data/glove.6B.zip\"\n",
    "#     response = requests.get(glove_url)\n",
    "#     z = zipfile.ZipFile(io.BytesIO(response.content))\n",
    "#     z.extractall()\n",
    "\n",
    "# download_glove()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: 'the'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m X_skipgram \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(sentence_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPreprocessed\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: average_word_vectors(x, skipgram_model, \u001b[38;5;241m100\u001b[39m))\u001b[38;5;241m.\u001b[39mtolist())\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgensim\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m KeyedVectors\n\u001b[0;32m----> 7\u001b[0m glove_model \u001b[38;5;241m=\u001b[39m \u001b[43mKeyedVectors\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_word2vec_format\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mglove.6B.100d.txt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbinary\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21maverage_word_vectors\u001b[39m(tokens, model, vector_size):\n\u001b[1;32m     10\u001b[0m     vector_sum \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros(vector_size)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/gensim/models/keyedvectors.py:1719\u001b[0m, in \u001b[0;36mKeyedVectors.load_word2vec_format\u001b[0;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype, no_header)\u001b[0m\n\u001b[1;32m   1672\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m   1673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_word2vec_format\u001b[39m(\n\u001b[1;32m   1674\u001b[0m         \u001b[38;5;28mcls\u001b[39m, fname, fvocab\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, binary\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf8\u001b[39m\u001b[38;5;124m'\u001b[39m, unicode_errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   1675\u001b[0m         limit\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, datatype\u001b[38;5;241m=\u001b[39mREAL, no_header\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m   1676\u001b[0m     ):\n\u001b[1;32m   1677\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Load KeyedVectors from a file produced by the original C word2vec-tool format.\u001b[39;00m\n\u001b[1;32m   1678\u001b[0m \n\u001b[1;32m   1679\u001b[0m \u001b[38;5;124;03m    Warnings\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1717\u001b[0m \n\u001b[1;32m   1718\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1719\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_load_word2vec_format\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1720\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfvocab\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfvocab\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbinary\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbinary\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43municode_errors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43municode_errors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1721\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlimit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlimit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatatype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdatatype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mno_header\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mno_header\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1722\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/gensim/models/keyedvectors.py:2059\u001b[0m, in \u001b[0;36m_load_word2vec_format\u001b[0;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype, no_header, binary_chunk_size)\u001b[0m\n\u001b[1;32m   2057\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2058\u001b[0m     header \u001b[38;5;241m=\u001b[39m utils\u001b[38;5;241m.\u001b[39mto_unicode(fin\u001b[38;5;241m.\u001b[39mreadline(), encoding\u001b[38;5;241m=\u001b[39mencoding)\n\u001b[0;32m-> 2059\u001b[0m     vocab_size, vector_size \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mint\u001b[39m(x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m header\u001b[38;5;241m.\u001b[39msplit()]  \u001b[38;5;66;03m# throws for invalid file format\u001b[39;00m\n\u001b[1;32m   2060\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m limit:\n\u001b[1;32m   2061\u001b[0m     vocab_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(vocab_size, limit)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/gensim/models/keyedvectors.py:2059\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   2057\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2058\u001b[0m     header \u001b[38;5;241m=\u001b[39m utils\u001b[38;5;241m.\u001b[39mto_unicode(fin\u001b[38;5;241m.\u001b[39mreadline(), encoding\u001b[38;5;241m=\u001b[39mencoding)\n\u001b[0;32m-> 2059\u001b[0m     vocab_size, vector_size \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m header\u001b[38;5;241m.\u001b[39msplit()]  \u001b[38;5;66;03m# throws for invalid file format\u001b[39;00m\n\u001b[1;32m   2060\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m limit:\n\u001b[1;32m   2061\u001b[0m     vocab_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(vocab_size, limit)\n",
      "\u001b[0;31mValueError\u001b[0m: invalid literal for int() with base 10: 'the'"
     ]
    }
   ],
   "source": [
    "cbow_model = Word2Vec(sentences=sentence_df['Preprocessed'].apply(lambda x: x.split()), vector_size=100, window=5, sg=0, min_count=1) \n",
    "X_cbow = np.array(sentence_df['Preprocessed'].apply(lambda x: average_word_vectors(x, cbow_model, 100)).tolist())\n",
    "skipgram_model = Word2Vec(sentences=sentence_df['Preprocessed'].apply(lambda x: x.split()), vector_size=100, window=5, sg=1, min_count=1) \n",
    "X_skipgram = np.array(sentence_df['Preprocessed'].apply(lambda x: average_word_vectors(x, skipgram_model, 100)).tolist())\n",
    "from gensim.models import KeyedVectors\n",
    "word2vec_glove_path = 'glove.6B.100d.txt'\n",
    "glove_model = KeyedVectors.load_word2vec_format(word2vec_glove_path, binary=False)\n",
    "def average_word(tokens, model, vector_size):\n",
    "    vector_sum = np.zeros(vector_size)\n",
    "    count = 0\n",
    "    for word in tokens:\n",
    "        if word in model:\n",
    "            vector_sum += model[word]\n",
    "            count += 1\n",
    "    if count != 0:\n",
    "        return vector_sum / count\n",
    "    else:\n",
    "        return np.zeros(vector_size)\n",
    "X_glove = np.array(sentence_df['Preprocessed'].apply(lambda x: average_word(x.split(), glove_model, 100)).tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import requests\n",
    "\n",
    "# model_url = \"https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.bin.gz\"\n",
    "# model_path = \"cc.en.300.bin.gz\"\n",
    "\n",
    "# response = requests.get(model_url)\n",
    "# with open(model_path, \"wb\") as file:\n",
    "#     file.write(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gzip\n",
    "# import shutil\n",
    "\n",
    "# with gzip.open('cc.en.300.bin.gz', 'rb') as f_in:\n",
    "#     with open('cc.en.300.bin', 'wb') as f_out:\n",
    "#         shutil.copyfileobj(f_in, f_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/k_/x2cc2f4901v6lyz_t7w3w3h40000gn/T/ipykernel_67620/4294752634.py:3: DeprecationWarning: Call to deprecated `load_fasttext_format` (use load_facebook_vectors (to use pretrained embeddings) or load_facebook_model (to continue training with the loaded full model, more RAM) instead).\n",
      "  gensim_model = FastText.load_fasttext_format(model_path_bin)\n",
      "/var/folders/k_/x2cc2f4901v6lyz_t7w3w3h40000gn/T/ipykernel_67620/4294752634.py:5: DeprecationWarning: Call to deprecated `load_fasttext_format` (use load_facebook_vectors (to use pretrained embeddings) or load_facebook_model (to continue training with the loaded full model, more RAM) instead).\n",
      "  gensim_model = FastText.load_fasttext_format(model_path_bin)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m gensim_model \u001b[38;5;241m=\u001b[39m FastText\u001b[38;5;241m.\u001b[39mload_fasttext_format(model_path_bin)\n\u001b[1;32m      4\u001b[0m X_fasttext \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(sentence_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPreprocessed\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: np\u001b[38;5;241m.\u001b[39mmean([gensim_model\u001b[38;5;241m.\u001b[39mwv[word] \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m x\u001b[38;5;241m.\u001b[39msplit() \u001b[38;5;28;01mif\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m gensim_model\u001b[38;5;241m.\u001b[39mwv], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m))\u001b[38;5;241m.\u001b[39mtolist())\n\u001b[0;32m----> 5\u001b[0m gensim_model \u001b[38;5;241m=\u001b[39m \u001b[43mFastText\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_fasttext_format\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path_bin\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/gensim/utils.py:1522\u001b[0m, in \u001b[0;36mdeprecated.<locals>.decorator.<locals>.new_func1\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mnew_func1\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1517\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   1518\u001b[0m         fmt\u001b[38;5;241m.\u001b[39mformat(name\u001b[38;5;241m=\u001b[39mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, reason\u001b[38;5;241m=\u001b[39mreason),\n\u001b[1;32m   1519\u001b[0m         category\u001b[38;5;241m=\u001b[39m\u001b[38;5;167;01mDeprecationWarning\u001b[39;00m,\n\u001b[1;32m   1520\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m\n\u001b[1;32m   1521\u001b[0m     )\n\u001b[0;32m-> 1522\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/gensim/models/fasttext.py:580\u001b[0m, in \u001b[0;36mFastText.load_fasttext_format\u001b[0;34m(cls, model_file, encoding)\u001b[0m\n\u001b[1;32m    568\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m    569\u001b[0m \u001b[38;5;129m@utils\u001b[39m\u001b[38;5;241m.\u001b[39mdeprecated(\n\u001b[1;32m    570\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124muse load_facebook_vectors (to use pretrained embeddings) or load_facebook_model \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    571\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m(to continue training with the loaded full model, more RAM) instead\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    572\u001b[0m )\n\u001b[1;32m    573\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_fasttext_format\u001b[39m(\u001b[38;5;28mcls\u001b[39m, model_file, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf8\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m    574\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Deprecated.\u001b[39;00m\n\u001b[1;32m    575\u001b[0m \n\u001b[1;32m    576\u001b[0m \u001b[38;5;124;03m    Use :func:`gensim.models.fasttext.load_facebook_model` or\u001b[39;00m\n\u001b[1;32m    577\u001b[0m \u001b[38;5;124;03m    :func:`gensim.models.fasttext.load_facebook_vectors` instead.\u001b[39;00m\n\u001b[1;32m    578\u001b[0m \n\u001b[1;32m    579\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 580\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mload_facebook_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/gensim/models/fasttext.py:728\u001b[0m, in \u001b[0;36mload_facebook_model\u001b[0;34m(path, encoding)\u001b[0m\n\u001b[1;32m    666\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_facebook_model\u001b[39m(path, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m    667\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Load the model from Facebook's native fasttext `.bin` output file.\u001b[39;00m\n\u001b[1;32m    668\u001b[0m \n\u001b[1;32m    669\u001b[0m \u001b[38;5;124;03m    Notes\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    726\u001b[0m \n\u001b[1;32m    727\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 728\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_load_fasttext_format\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfull_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/gensim/models/fasttext.py:844\u001b[0m, in \u001b[0;36m_load_fasttext_format\u001b[0;34m(model_file, encoding, full_model)\u001b[0m\n\u001b[1;32m    840\u001b[0m model\u001b[38;5;241m.\u001b[39mprepare_vocab(update\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, min_count\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    842\u001b[0m model\u001b[38;5;241m.\u001b[39mnum_original_vectors \u001b[38;5;241m=\u001b[39m m\u001b[38;5;241m.\u001b[39mvectors_ngrams\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m--> 844\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit_post_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvectors_ngrams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    845\u001b[0m model\u001b[38;5;241m.\u001b[39m_init_post_load(m\u001b[38;5;241m.\u001b[39mhidden_output)\n\u001b[1;32m    847\u001b[0m _check_model(model)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/gensim/models/fasttext.py:1189\u001b[0m, in \u001b[0;36mFastTextKeyedVectors.init_post_load\u001b[0;34m(self, fb_vectors)\u001b[0m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvectors_ngrams \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(fb_vectors[vocab_words:, :])\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecalc_char_ngram_buckets()\n\u001b[0;32m-> 1189\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madjust_vectors\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/gensim/models/fasttext.py:1207\u001b[0m, in \u001b[0;36mFastTextKeyedVectors.adjust_vectors\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1205\u001b[0m ngram_buckets \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuckets_word[i]\n\u001b[1;32m   1206\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m nh \u001b[38;5;129;01min\u001b[39;00m ngram_buckets:\n\u001b[0;32m-> 1207\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvectors[i] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvectors_ngrams[nh]\n\u001b[1;32m   1208\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvectors[i] \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(ngram_buckets) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from gensim.models import FastText\n",
    "model_path_bin = 'cc.en.300.bin'\n",
    "gensim_model = FastText.load_fasttext_format(model_path_bin)\n",
    "X_fasttext = np.array(sentence_df['Preprocessed'].apply(lambda x: np.mean([gensim_model.wv[word] for word in x.split() if word in gensim_model.wv], axis=0)).tolist())\n",
    "gensim_model = FastText.load_fasttext_format(model_path_bin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Another high-profile financial firm, China Life, is facing shareholder\n",
      "lawsuits and a probe by the US Securities and Exchange Commission\n",
      "following its 2004 New York listing over its failure to disclose\n",
      "accounting irregularities at its parent company. However, a report in\n",
      "the China-published Economic Observer said on Monday that the two\n",
      "banks may have scrapped plans to list in New York because of the cost\n",
      "of meeting regulatory requirements imposed since the Enron scandal.\n",
      "Last year, China's premier, Wen Jiabao, targeted bank lending\n",
      "practices in a series of speeches, and regulators ordered all big\n",
      "loans to be scrutinized, in an attempt to cool down irresponsible\n",
      "lending. Millions go missing at China bank Two senior officials at one\n",
      "of China's top commercial banks have reportedly disappeared after\n",
      "funds worth up to $120m (Â£64m) went missing. China's banking sector is\n",
      "burdened with at least $190bn of bad debt according to official data,\n",
      "though most observers believe the true figure is far higher.\n"
     ]
    }
   ],
   "source": [
    "sentence_importance_bow = X_bow.sum(axis=1)\n",
    "sentence_df['importance_bow'] = sentence_importance_bow\n",
    "top_sentences_bow = sentence_df.sort_values('importance_bow',\n",
    "ascending=False).head(5)['Original'].tolist()\n",
    "summary_bow = ' '.join(top_sentences_bow)\n",
    "print(textwrap.fill(summary_bow))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Another high-profile financial firm, China Life, is facing shareholder\n",
      "lawsuits and a probe by the US Securities and Exchange Commission\n",
      "following its 2004 New York listing over its failure to disclose\n",
      "accounting irregularities at its parent company. However, a report in\n",
      "the China-published Economic Observer said on Monday that the two\n",
      "banks may have scrapped plans to list in New York because of the cost\n",
      "of meeting regulatory requirements imposed since the Enron scandal.\n",
      "Last year, China's premier, Wen Jiabao, targeted bank lending\n",
      "practices in a series of speeches, and regulators ordered all big\n",
      "loans to be scrutinized, in an attempt to cool down irresponsible\n",
      "lending. China's banking sector is burdened with at least $190bn of\n",
      "bad debt according to official data, though most observers believe the\n",
      "true figure is far higher. Millions go missing at China bank Two\n",
      "senior officials at one of China's top commercial banks have\n",
      "reportedly disappeared after funds worth up to $120m (Â£64m) went\n",
      "missing.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "sentence_importance_tfidf = X_tfidf.sum(axis=1)\n",
    "sentence_df['importance_tfidf'] = sentence_importance_tfidf\n",
    "top_sentences_tfidf = sentence_df.sort_values('importance_tfidf',\n",
    "ascending=False).head(5)['Original'].tolist()\n",
    "          \n",
    "summary_tfidf = ' '.join(top_sentences_tfidf)\n",
    "print(textwrap.fill(summary_tfidf))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Millions go missing at China bank Two senior officials at one of\n",
      "China's top commercial banks have reportedly disappeared after funds\n",
      "worth up to $120m (Â£64m) went missing. The pair both worked at Bank of\n",
      "China in the northern city of Harbin, the South China Morning Post\n",
      "reported. Second, many questionable loans come not from the big four,\n",
      "but from smaller banks. First, inefficient state enterprises continue\n",
      "to receive protection from bankruptcy because they employ large\n",
      "numbers of people. China's leaders see reforming the top four banks as\n",
      "vital to distribute capital to profitable companies and protect the\n",
      "health of China's economic boom.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "sentence_importance_cbow = X_cbow.sum(axis=1)\n",
    "sentence_df['importance_cbow'] = sentence_importance_cbow\n",
    "top_sentences_cbow = sentence_df.sort_values('importance_cbow',\n",
    "ascending=False).head(5)['Original'].tolist()\n",
    "summary_cbow = ' '.join(top_sentences_cbow)\n",
    "print(textwrap.fill(summary_cbow))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Millions go missing at China bank Two senior officials at one of\n",
      "China's top commercial banks have reportedly disappeared after funds\n",
      "worth up to $120m (Â£64m) went missing. The pair both worked at Bank of\n",
      "China in the northern city of Harbin, the South China Morning Post\n",
      "reported. Second, many questionable loans come not from the big four,\n",
      "but from smaller banks. First, inefficient state enterprises continue\n",
      "to receive protection from bankruptcy because they employ large\n",
      "numbers of people. China's leaders see reforming the top four banks as\n",
      "vital to distribute capital to profitable companies and protect the\n",
      "health of China's economic boom.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "sentence_importance_skipgram = X_skipgram.sum(axis=1)\n",
    "sentence_df['importance_skipgram'] = sentence_importance_skipgram\n",
    "      \n",
    "top_sentences_skipgram = sentence_df.sort_values('importance_skipgram', ascending=False).head(5)['Original'].tolist()\n",
    "summary_skipgram = ' '.join(top_sentences_skipgram)\n",
    "print(textwrap.fill(summary_skipgram))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'glove_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     11\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mzeros(vector_size)\n\u001b[0;32m---> 13\u001b[0m X_glove \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(\u001b[43msentence_df\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mPreprocessed\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43maverage_word\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mglove_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtolist())\n\u001b[1;32m     14\u001b[0m sentence_importance_glove \u001b[38;5;241m=\u001b[39m X_glove\u001b[38;5;241m.\u001b[39msum(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     15\u001b[0m sentence_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimportance_glove\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m sentence_importance_glove\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/core/series.py:4757\u001b[0m, in \u001b[0;36mSeries.apply\u001b[0;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[1;32m   4629\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[1;32m   4630\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   4631\u001b[0m     func: AggFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4636\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   4637\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[1;32m   4638\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   4639\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[1;32m   4640\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4755\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[1;32m   4756\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 4757\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4758\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4759\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4760\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4761\u001b[0m \u001b[43m        \u001b[49m\u001b[43mby_row\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mby_row\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4762\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4763\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4764\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/core/apply.py:1209\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1206\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_compat()\n\u001b[1;32m   1208\u001b[0m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[0;32m-> 1209\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/core/apply.py:1289\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1283\u001b[0m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[1;32m   1284\u001b[0m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[1;32m   1285\u001b[0m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[1;32m   1286\u001b[0m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[1;32m   1287\u001b[0m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[1;32m   1288\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj\u001b[38;5;241m.\u001b[39mdtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1289\u001b[0m mapped \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_values\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmapper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurried\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\n\u001b[1;32m   1291\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1293\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[1;32m   1294\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[1;32m   1295\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[1;32m   1296\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/core/base.py:921\u001b[0m, in \u001b[0;36mIndexOpsMixin._map_values\u001b[0;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m    918\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[1;32m    919\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mmap(mapper, na_action\u001b[38;5;241m=\u001b[39mna_action)\n\u001b[0;32m--> 921\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43malgorithms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/core/algorithms.py:1814\u001b[0m, in \u001b[0;36mmap_array\u001b[0;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m   1812\u001b[0m values \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m   1813\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1814\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1815\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1816\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer_mask(\n\u001b[1;32m   1817\u001b[0m         values, mapper, mask\u001b[38;5;241m=\u001b[39misna(values)\u001b[38;5;241m.\u001b[39mview(np\u001b[38;5;241m.\u001b[39muint8), convert\u001b[38;5;241m=\u001b[39mconvert\n\u001b[1;32m   1818\u001b[0m     )\n",
      "File \u001b[0;32mlib.pyx:2926\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "Cell \u001b[0;32mIn[16], line 13\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     11\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mzeros(vector_size)\n\u001b[0;32m---> 13\u001b[0m X_glove \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(sentence_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPreprocessed\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: average_word(x\u001b[38;5;241m.\u001b[39msplit(), \u001b[43mglove_model\u001b[49m, \u001b[38;5;241m100\u001b[39m))\u001b[38;5;241m.\u001b[39mtolist())\n\u001b[1;32m     14\u001b[0m sentence_importance_glove \u001b[38;5;241m=\u001b[39m X_glove\u001b[38;5;241m.\u001b[39msum(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     15\u001b[0m sentence_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimportance_glove\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m sentence_importance_glove\n",
      "\u001b[0;31mNameError\u001b[0m: name 'glove_model' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "def average_word(tokens, model, vector_size):\n",
    "    vector_sum = np.zeros(vector_size)\n",
    "    count = 0\n",
    "    for word in tokens:\n",
    "        if word in model.wv:\n",
    "            vector_sum += model.wv[word]\n",
    "            count += 1\n",
    "    if count != 0:\n",
    "        return vector_sum / count\n",
    "    else:\n",
    "        return np.zeros(vector_size)\n",
    "\n",
    "X_glove = np.array(sentence_df['Preprocessed'].apply(lambda x: average_word(x.split(), glove_model, 100)).tolist())\n",
    "sentence_importance_glove = X_glove.sum(axis=1)\n",
    "sentence_df['importance_glove'] = sentence_importance_glove\n",
    "top_sentences_glove = sentence_df.sort_values('importance_glove',\n",
    "ascending=False).head(5)['Original'].tolist()\n",
    "summary_glove = ' '.join(top_sentences_glove)\n",
    "print(textwrap.fill(summary_glove))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sentence_importance_fasttext = X_fasttext.sum(axis=1)\n",
    "sentence_df['importance_fasttext'] = sentence_importance_fasttext\n",
    "top_sentences_fasttext = sentence_df.sort_values('importance_fasttext',\n",
    "ascending=False).head(5)['Original'].tolist()\n",
    "          \n",
    "summary_fasttext = ' '.join(top_sentences_fasttext)\n",
    "print(textwrap.fill(summary_fasttext))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
